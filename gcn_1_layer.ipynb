{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import spektral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jianr\\miniconda3\\lib\\site-packages\\scipy\\sparse\\_index.py:112: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "dataset = spektral.datasets.Citation(\"cora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix = dataset[0].a.todense() + np.eye(dataset[0].n_nodes) # add self loop\n",
    "adj_matrix = adj_matrix.astype('float32') # 2708 * 2708 # citation relationships\n",
    "node_features = dataset[0].x # 2708 * 1433, 1433 terms noted for each paper\n",
    "node_labels = dataset[0].y # 2708 * 7, 7 classes for node classification\n",
    "train_mask = dataset.mask_tr\n",
    "val_mask = dataset.mask_va\n",
    "test_mask = dataset.mask_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy(logits, labels, mask):\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, labels, mask):\n",
    "    # equivalent expression: np.sum(accuracy_all*mask)/np.sum(mask)\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask) # adjust mask proportion to sum up to 2708\n",
    "    accuracy_all *= mask # apply mask to accuracy\n",
    "    return tf.reduce_mean(accuracy_all) # eq. sum of accuracy / 2708"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnn(node_features, adj_matrix, transform, activation):\n",
    "    # weight first, then aggregate neighbor features\n",
    "    #weighted_features = transform(node_features) # apply weights to node features\n",
    "    #aggregated_features = tf.matmul(adj_matrix, weighted_features) # aggregated neighbor node features\n",
    "    #return activation(aggregated_features) # apply activation function\n",
    "    # aggregate neighbor features first, then weight\n",
    "    aggregated_features = tf.matmul(adj_matrix, node_features) # aggregated neighbor node features\n",
    "    weighted_features = transform(aggregated_features) # apply weights to node features\n",
    "    return activation(weighted_features) # apply activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cora(node_features, adj_matrix, gnn, hidden_units, epochs, learning_rate):\n",
    "    layer_1 = tf.keras.layers.Dense(hidden_units) # hidden layer: apply weights\n",
    "    layer_2 = tf.keras.layers.Dense(7) # 7 classes for nodes\n",
    "    \n",
    "    def cora_gnn(node_features, adj_matrix):\n",
    "        hidden = gnn(node_features, adj_matrix, layer_1, tf.nn.relu)\n",
    "        logits = gnn(hidden, adj_matrix, layer_2, tf.identity)\n",
    "        return logits\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate) # use Adam optimizer\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    for ep in range(epochs + 1):\n",
    "        with tf.GradientTape() as t:\n",
    "            logits = cora_gnn(node_features, adj_matrix)\n",
    "            loss = softmax_cross_entropy(logits, node_labels, train_mask)\n",
    "            variables = t.watched_variables()\n",
    "            grads = t.gradient(loss, variables)\n",
    "            optimizer.apply_gradients(zip(grads, variables)) # apply gradients to variables\n",
    "            val_accuracy = accuracy(logits, node_labels, val_mask)\n",
    "            test_accuracy = accuracy(logits, node_labels, test_mask)\n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_accuracy = val_accuracy\n",
    "                print('Epoch', ep, '| Training loss:', loss.numpy(), '| Val accuracy:'\n",
    "                      , val_accuracy.numpy(), '| Test accuracy', test_accuracy.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Training loss: 4.304349 | Val accuracy: 0.11999998 | Test accuracy 0.118\n",
      "Epoch 1 | Training loss: 12.151947 | Val accuracy: 0.38999996 | Test accuracy 0.399\n",
      "Epoch 2 | Training loss: 18.13162 | Val accuracy: 0.40199998 | Test accuracy 0.417\n",
      "Epoch 3 | Training loss: 9.70142 | Val accuracy: 0.52 | Test accuracy 0.546\n",
      "Epoch 4 | Training loss: 5.537958 | Val accuracy: 0.648 | Test accuracy 0.63600004\n",
      "Epoch 8 | Training loss: 1.39016 | Val accuracy: 0.65 | Test accuracy 0.662\n",
      "Epoch 9 | Training loss: 1.1165088 | Val accuracy: 0.678 | Test accuracy 0.674\n",
      "Epoch 16 | Training loss: 0.2278719 | Val accuracy: 0.686 | Test accuracy 0.70600003\n",
      "Epoch 17 | Training loss: 0.16955851 | Val accuracy: 0.714 | Test accuracy 0.735\n",
      "Epoch 18 | Training loss: 0.10948553 | Val accuracy: 0.726 | Test accuracy 0.754\n",
      "Epoch 19 | Training loss: 0.06772305 | Val accuracy: 0.728 | Test accuracy 0.7569999\n",
      "Epoch 20 | Training loss: 0.063029215 | Val accuracy: 0.732 | Test accuracy 0.76699996\n",
      "Epoch 21 | Training loss: 0.06477899 | Val accuracy: 0.734 | Test accuracy 0.76399994\n",
      "Epoch 22 | Training loss: 0.065235466 | Val accuracy: 0.736 | Test accuracy 0.75899994\n",
      "Epoch 50 | Training loss: 0.0031925773 | Val accuracy: 0.738 | Test accuracy 0.73999995\n",
      "Epoch 61 | Training loss: 0.0018262557 | Val accuracy: 0.73800004 | Test accuracy 0.73800004\n",
      "Epoch 62 | Training loss: 0.0017578619 | Val accuracy: 0.74 | Test accuracy 0.73800004\n",
      "Epoch 66 | Training loss: 0.0015300817 | Val accuracy: 0.742 | Test accuracy 0.739\n"
     ]
    }
   ],
   "source": [
    "# GNN: A H W\n",
    "train_cora(node_features, adj_matrix, gnn, 100, 200, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Training loss: 1.9443457 | Val accuracy: 0.222 | Test accuracy 0.20700002\n",
      "Epoch 1 | Training loss: 1.5881351 | Val accuracy: 0.69 | Test accuracy 0.7289999\n",
      "Epoch 2 | Training loss: 1.1884322 | Val accuracy: 0.72599995 | Test accuracy 0.75899994\n",
      "Epoch 3 | Training loss: 0.8102507 | Val accuracy: 0.76799995 | Test accuracy 0.774\n",
      "Epoch 4 | Training loss: 0.52205074 | Val accuracy: 0.784 | Test accuracy 0.8039998\n",
      "Epoch 5 | Training loss: 0.3270284 | Val accuracy: 0.786 | Test accuracy 0.8109998\n"
     ]
    }
   ],
   "source": [
    "# GCN: D^-0.5 A D^-0.5 H W\n",
    "deg_matrix = tf.reduce_sum(adj_matrix, axis=-1)\n",
    "norm_deg_matrix = tf.linalg.diag(1.0 / tf.sqrt(deg_matrix))\n",
    "norm_adj_matrix = tf.matmul(norm_deg_matrix, tf.matmul(adj_matrix, norm_deg_matrix))\n",
    "train_cora(node_features, norm_adj_matrix, gnn, 100, 200, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is a customized GCN Model capsulating the learning process above. However, this model was buggy so I just leave it here to show the concept. A working version of the model was implemented in the notebook named gcn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_units,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(GraphConvLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.update_fn = tf.keras.layers.Dense(hidden_units)\n",
    "        self.hidden_units = hidden_units\n",
    "\n",
    "    def aggregate(self, node_features, adj_matrix):\n",
    "        # aggregated neighbor node features\n",
    "        aggregated_features = tf.matmul(adj_matrix, node_features)\n",
    "        \n",
    "        return aggregated_features\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Process the inputs to produce the node_embeddings.\n",
    "\n",
    "        inputs: a tuple of three elements: node_repesentations, edges, edge_weights.\n",
    "        Returns: node_embeddings of shape [num_nodes, representation_dim].\n",
    "        \"\"\"\n",
    "        print(\"num:\", self.hidden_units)\n",
    "        node_repesentations, adj_matrix = inputs\n",
    "        # Aggregate the neighbour messages.\n",
    "        aggregated_features = self.aggregate(node_repesentations, adj_matrix)\n",
    "        # Update the node embedding with the weights.\n",
    "        return self.update_fn(aggregated_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNNodeClassifier(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        graph_info,\n",
    "        num_classes,\n",
    "        hidden_units,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(GCNNodeClassifier, self).__init__(*args, **kwargs)\n",
    "\n",
    "        # Unpack graph_info to three elements: node_features, edges, and edge_weight.\n",
    "        node_features, adj_matrix = graph_info\n",
    "        self.node_features = node_features\n",
    "        self.adj_matrix = adj_matrix\n",
    "\n",
    "        # Create the convoluted layer.\n",
    "        self.conv = tf.keras.layers.Dense(hidden_units, name=\"graph_conv1\")\n",
    "        # Create a compute logits layer.\n",
    "        self.compute_logits = tf.keras.layers.Dense(num_classes, name=\"logits\")\n",
    "\n",
    "    def call(self, input_node_indices):\n",
    "        # GCN layer.\n",
    "        x = self.conv((self.node_features))\n",
    "        # Classification layer\n",
    "        x1 = self.compute_logits((x))\n",
    "        # Fetch node embeddings for the input node_indices.\n",
    "        node_embeddings = tf.squeeze(tf.gather(x1, input_node_indices))\n",
    "        return node_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN output shape: tf.Tensor(\n",
      "[[-0.32148695  0.10195416 -0.5454991   0.0115348  -0.08195534 -0.21125859\n",
      "   0.15940002]\n",
      " [ 0.16814412 -0.03495014 -0.06088027 -0.2042849  -0.11284803  0.21043402\n",
      "  -0.02413301]\n",
      " [-0.13113461  0.02389363 -0.10501588 -0.12333894  0.08416583  0.24914908\n",
      "  -0.05936877]], shape=(3, 7), dtype=float32)\n",
      "Model: \"gnn_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "graph_conv1 (Dense)          multiple                  45888     \n",
      "_________________________________________________________________\n",
      "logits (Dense)               multiple                  231       \n",
      "=================================================================\n",
      "Total params: 46,119\n",
      "Trainable params: 46,119\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gnn_model = GCNNodeClassifier(\n",
    "    graph_info=(node_features, adj_matrix),\n",
    "    num_classes=7,\n",
    "    hidden_units=32,\n",
    "    name=\"gnn_model\",\n",
    ")\n",
    "\n",
    "print(\"GNN output shape:\", gnn_model([1, 10, 100]))\n",
    "\n",
    "gnn_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
